#  Kafka â†’ ClickHouse Pipeline (Local Docker Setup)

This folder contains a **local version** of the real-time pipeline using **Docker Compose**.
It runs the following services:

* **Zookeeper** â†’ Required by Kafka.
* **Kafka** â†’ Message broker where events are published.
* **ClickHouse** â†’ Database with Kafka Engine for real-time ingestion.
* **Producer** â†’ Python app that generates mock user signup events.

---

## ğŸ“‚ Folder Structure

```
docker/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ clickhouse-init/
    â””â”€â”€ init.sql
```

---

## âš™ï¸ How It Works

1. **Producer** pushes messages into Kafka topic `user-signups`.
2. **ClickHouse Kafka Engine Table** subscribes to the topic.
3. **Materialized View** auto-inserts data into `demo.user_signups` table.
4. You can query live data directly in ClickHouse.

---

## ğŸ› ï¸ Steps to Run

1. **Clone repo & go into docker setup**

   ```bash
   git clone https://github.com/mohhddhassan/kafka-clickhouse-k8s.git
   cd kafka-clickhouse-pipeline/docker
   ```

2. **Start services**

   ```bash
   docker compose up -d
   ```

3. **Verify tables in ClickHouse**

   ```bash
   curl "http://localhost:8123/?query=SHOW+TABLES+FROM+demo"
   ```

   Expected output:

   ```
   kafka_signups
   user_signups
   signup_mv
   ```

4. **Check data flowing in**

   ```bash
   curl "http://localhost:8123/?query=SELECT+*+FROM+demo.user_signups+LIMIT+5"
   ```

   Example:

   ```
   123   2025-09-16 12:34:56
   456   2025-09-16 12:34:58
   ```

---

## ğŸ›‘ Stopping Services

```bash
docker compose down
```

---

## ğŸ“Œ Notes

* Producer generates a random `user_id` every 2 seconds.
* `init.sql` runs automatically when ClickHouse starts, creating all required tables.
* This setup is **only for local testing**. For production-like deployment â†’ check the `k8s/` folder.

---
